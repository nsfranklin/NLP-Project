{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPL Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries, Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as Skmet #used for precision_recall_fscore_support()\n",
    "from operator import itemgetter #used to unpack turples\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = text.lower() #normalize the text \n",
    "    text = re.sub(r\"<.*>\", \" \", text) # speach mark removal.\n",
    "    text = re.sub(r\".\\'\", \"\" , text) # to prevent 'don't' becoming 'don' 't'\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    preNormTemp = tokenizer.tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english')) #stop word removal.\n",
    "    \n",
    "    \n",
    "    for w in preNormTemp: #stop word removal and lemmatization\n",
    "        if (w not in stopWords):\n",
    "            temp.append(w)  #lemmatizer.lemmatize(w))\n",
    "        if \n",
    "    temp = text.split(\" \") \n",
    "    \n",
    "    return preNormTemp\n",
    "\n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        count = 0\n",
    "        for line in reader:\n",
    "            if count == 0:\n",
    "                count += 1\n",
    "            else:\n",
    "                (line, character, gender) = parseReview(line)\n",
    "                rawText.append(line, character, gender);\n",
    "                preprocessedData.append((preProcess(line), character, gender))\n",
    "                count += 1\n",
    "        \n",
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    \n",
    "    for (line, chatacter, Label, Verified, Title) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text),preProcess(Title),Verified),Label))\n",
    "    \n",
    "    for (_, Text, Label, Verified, Title) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text),preProcess(Title),Verified),Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ain', 'on', \"she's\", 'as', \"wouldn't\", \"you're\", 'each', 'ourselves', 'ours', \"mustn't\", 'does', 'not', 'i', 'what', 'then', 'have', 'during', 'until', 'about', 't', 'than', 'when', \"hasn't\", 'most', 'at', 'here', 'too', 'will', 'y', 'won', \"weren't\", 'itself', 'more', 're', 'you', 'by', 'before', 'can', 'over', \"won't\", 'because', 'them', 'd', 'in', 'these', 'did', 'under', 'very', 'only', 'she', 'has', 'out', 'who', 'such', 'down', 'doing', 'your', 'up', 'couldn', 'hers', 'we', \"doesn't\", 'aren', 'are', 'further', 'of', 'again', 'themselves', \"you'd\", 'few', 'other', 'ma', 'both', 'a', 'having', 'doesn', 'was', 'off', 'just', 'mustn', 'isn', 'any', \"wasn't\", 'haven', 'for', \"you'll\", \"shouldn't\", 'through', \"don't\", 'yours', 'now', 'our', 'shan', 'him', 'an', 'why', 'my', \"aren't\", 'while', \"it's\", 'himself', 'theirs', 'nor', 'mightn', 'myself', 'do', 'his', 'it', 'their', 'into', 'is', 'how', 'own', 'he', \"should've\", 'with', 'if', 'against', 'm', 'some', 'wouldn', 'or', 'its', \"didn't\", 'so', 'but', \"that'll\", \"isn't\", 'yourselves', 'which', 'should', 'me', 'from', \"needn't\", 'there', 'that', 's', 'and', 'weren', 'herself', 'wasn', 'between', 'had', 'they', 'were', 'o', 'll', 'the', 'be', 'all', 'after', 'no', \"haven't\", 'same', 'been', 'those', 'whom', \"mightn't\", 'her', 'needn', \"hadn't\", 'below', 've', 'don', 'above', \"couldn't\", 'didn', 'where', 'once', 'hadn', 'this', 'to', 'shouldn', 'being', 'yourself', 'hasn', \"you've\", \"shan't\", 'am'}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #print(testingSetRemovedLabel)\n",
    "        #training\n",
    "        classifier = trainClassifier(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabel(i,classifier))\n",
    "        \n",
    "        #predictLabels using 1/10th of the dataset\n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,pos_label=\"fake\") #finds precision score\n",
    "        recall = Skmet.recall_score(trueLabels, results,pos_label=\"fake\") #finds recall score\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
