{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPL Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries, Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # csv reader\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as Skmet #used for precision_recall_fscore_support()\n",
    "from operator import itemgetter #used to unpack turples\n",
    "import re\n",
    "import math\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(dataLine):\n",
    "    line = dataLine[0]     \n",
    "    character = dataLine[1] \n",
    "    gender = dataLine[2]   \n",
    "    return (line, character, gender)\n",
    "\n",
    "def parseContractions(dataLine):\n",
    "    contraction = dataLine[0]     \n",
    "    expansion = dataLine[1]    \n",
    "    return (contraction.lower(), expansion.lower())\n",
    "\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = text.lower() #normalize the text \n",
    "    text = re.sub(r\"\\\"\", \"\", text) # speach mark removal.\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[']?\\w+[']?\\w?\\w?\") # splits sent into words maintain  \" ' \" to allow for later expansion.\n",
    "    tokenizedTemp = tokenizer.tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english')) #stop word removal.\n",
    "    \n",
    "    contractionExpandedTemp = []\n",
    "    for w in tokenizedTemp:\n",
    "        contemp = contractions.get(w)\n",
    "        if  contemp != None:\n",
    "            tempContract = contractions[w] #getting contraction dictionary value\n",
    "            expandedContraction = tempContract.split(\" \") #splits expanded cotnractions then adds it to the processed text.\n",
    "            contractionExpandedTemp.extend(expandedContraction);\n",
    "        else:\n",
    "            contractionExpandedTemp.append(w)\n",
    "    processedTemp = []\n",
    "    \n",
    "    for w in contractionExpandedTemp: #stop word removal and lemmatization\n",
    "        if (w not in stopWords) or w == \"\": # removes stop words and blank strings\n",
    "            processedTemp.append(w)  #lemmatizer.lemmatize(w))   \n",
    "    return processedTemp\n",
    "\n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for line in reader:\n",
    "            (line, character, gender) = parseData(line)\n",
    "            temp = preProcess(line)\n",
    "            #if not temp: #removes empty lines. Either fully trimmed or original empty.\n",
    "            rawData.append((line, character, gender));  \n",
    "            preprocessedData.append((preProcess(line), character, gender))\n",
    "                \n",
    "def loadContractions(path):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            (contraction, expansion) = parseContractions(line)\n",
    "            contractions[contraction] = expansion\n",
    "            \n",
    "def splitData(percentage): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "        \n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "        \n",
    "def toFeatureVector(tokens):\n",
    "    lineDict = {} # Should return a dictionary containing features as keys, and weights as values\n",
    "    for token in tokens: #adding to the line dict\n",
    "        if token in lineDict:\n",
    "            lineDict[token] = lineDict[token] + 1\n",
    "        else:\n",
    "            lineDict[token] = 1\n",
    "            \n",
    "    for token in tokens: #adds values to featureDict\n",
    "        if token in featureDict:\n",
    "            featureDict[token] = featureDict[token] + 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "    \n",
    "    return lineDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #training\n",
    "        classifier = trainClassifier(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabel(i,classifier))\n",
    "        \n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,average='macro') #finds precision score\n",
    "        #print(precision)\n",
    "        recall = Skmet.recall_score(trueLabels, results,average='macro') #finds recall score\n",
    "        #print(recall)\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        #print(f_score)\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        #print(accuracy)\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify((reviewSample))\n",
    "\n",
    "def word2vecInitModel():\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    word2vec = Word2Vec(min_count=5, #the kaggle tutorial used a much larger dataset \n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "    return word2vec\n",
    "\n",
    "def word2vecVocabandTraining(dataset, model)\n",
    "    t = time()\n",
    "    model.build_vocab(dataset, progress_per=1000)\n",
    "    print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    \n",
    "    t = time();\n",
    "    model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "    print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData, 0 trainData, 0 testData\n",
      "Preparing the datasets...\n",
      "Now 10113 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 10113 rawData, 9100 trainData, 1013 testData\n",
      "Training Samples: \n",
      "9100\n",
      "Features: \n",
      "5754\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Precision, Recall, Fscore, Accuracy\n",
      "(0.552863688427898, 0.5525422108455417, 0.5527028606377715, 0.5524175824175823)\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Precision, Recall, Fscore, Accuracy\n",
      "(0.17559268036085093, 0.1614969298891055, 0.16823929530879572, 0.19098901098901097)\n"
     ]
    }
   ],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "contractions = {} #dictionary of english contractions\n",
    "rawData = []          # the filtered data from the dataset file\n",
    "preprocessedData = [] # the preprocessed reviews \n",
    "trainDataBinary = []   # the training data with the binary gender labels.\n",
    "testDataBinary = [] # the test data currently 10% with the binary gender labels.\n",
    "trainDataMulti = [] # the training data with the multi class name labels.\n",
    "testDataMulti = [] # the test data currently 10% with the multi class name labels.\n",
    "\n",
    "trainingDataSource = \"training.csv\"\n",
    "testingDataSource = \"test.csv\"\n",
    "contractionsPath = \"contractions.txt\"\n",
    "loadContractions(contractionsPath)\n",
    "\n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData, %d trainData, %d testData\" % (len(rawData), len(trainDataMulti), len(testDataMulti),len(trainDataBinary), len(testDataBinary)),\"Preparing the datasets...\",sep='\\n')\n",
    "loadData(trainingDataSource)  \n",
    "\n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainDataBinary), len(testDataBinary)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.9)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainDataBinary), len(testDataBinary)),\n",
    "      \"Training Samples: \", len(trainDataBinary), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "\n",
    "\n",
    "#print(featureDict)\n",
    "#print(trainDataBinary)\n",
    "#print(trainDataMulti)\n",
    "\n",
    "\n",
    "crossValidationBinaryResults = crossValidate(trainDataBinary, 10)\n",
    "print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "print(crossValidationBinaryResults)\n",
    "\n",
    "crossValidationMultiResults = crossValidate(trainDataMulti, 10)\n",
    "\n",
    "print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "print(crossValidationMultiResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are the results on a simple SVM \n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.552863688427898, 0.5525422108455417, 0.5527028606377715, 0.5524175824175823)\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.17559268036085093, 0.1614969298891055, 0.16823929530879572, 0.19098901098901097)\n",
    "##### Here are the results with the edition of Word2Vec Features\n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
