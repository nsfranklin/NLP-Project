{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPL Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries, Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to c:/nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to c:/nltk_data/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to c:/nltk_data/...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to c:/nltk_data/...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv # csv reader\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import tree2conlltags\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as Skmet #used for precision_recall_fscore_support()\n",
    "from operator import itemgetter #used to unpack turples\n",
    "import re\n",
    "import math\n",
    "import multiprocessing\n",
    "#from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download(\"wordnet\", \"c:/nltk_data/\")\n",
    "nltk.download('stopwords', \"c:/nltk_data/\")\n",
    "nltk.download('maxent_ne_chunker', \"c:/nltk_data/\")\n",
    "nltk.download('words', \"c:/nltk_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(dataLine):\n",
    "    line = dataLine[0]     \n",
    "    character = dataLine[1] \n",
    "    gender = dataLine[2]   \n",
    "    return (line, character, gender)\n",
    "\n",
    "def parseContractions(dataLine):\n",
    "    contraction = dataLine[0]     \n",
    "    expansion = dataLine[1]  \n",
    "    return (contraction, expansion)\n",
    "\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = text.lower() #normalize the text \n",
    "    text = re.sub(r\"\\\"\", \"\", text) # speach mark removal.\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[']?\\w+[']?\\w?\\w?\\w?\\w?\\w?\") # splits sent into words maintain  \" ' \" to allow for later expansion.\n",
    "    tokenizedTemp = tokenizer.tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english')) #stop word removal.\n",
    "    \n",
    "    contractionExpandedTemp = []\n",
    "    for w in tokenizedTemp:\n",
    "        contemp = contractions.get(w)\n",
    "        if  contemp != None:\n",
    "            tempContract = contractions[w] #getting contraction dictionary value\n",
    "            expandedContraction = tempContract.split(\" \") #splits expanded cotnractions then adds it to the processed text.\n",
    "            contractionExpandedTemp.extend(expandedContraction);\n",
    "        else:\n",
    "            contractionExpandedTemp.append(w)\n",
    "    \n",
    "    processedTemp = []\n",
    "    for w in contractionExpandedTemp: #stop word removal and lemmatization\n",
    "        if (w not in stopWords) or w == \"\": # removes stop words and blank strings\n",
    "            processedTemp.append(w)   \n",
    "    \n",
    "    taggedTemp = nltk.pos_tag(processedTemp)\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\" #nltk documentation.\n",
    "        \n",
    "    if taggedTemp:\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        result = cp.parse(taggedTemp)\n",
    "        IOBtagged = tree2conlltags(result) #converts a tree to a series of turples.\n",
    "        return IOBtagged\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for line in reader:\n",
    "            (line, character, gender) = parseData(line)\n",
    "            temp = preProcess(line)\n",
    "            if temp: #removes empty lines\n",
    "                rawData.append((line, character, gender));  \n",
    "                preprocessedData.append((temp, character, gender))\n",
    "\n",
    "def loadContractions(path):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            (contraction, expansion) = parseContractions(line)\n",
    "            contractions[contraction] = expansion\n",
    "            \n",
    "def splitData(percentage): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "        \n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "        \n",
    "def toFeatureVector(tokens):\n",
    "    lineDict = {} # Should return a dictionary containing features as keys, and weights as values\n",
    "    for token in tokens: #adding to the line dict\n",
    "        if token in lineDict:\n",
    "            lineDict[token] = lineDict[token] + 1\n",
    "        else:\n",
    "            lineDict[token] = 1\n",
    "            \n",
    "    for token in tokens: #adds values to featureDict\n",
    "        if token in featureDict:\n",
    "            featureDict[token] = featureDict[token] + 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "        \n",
    "    return lineDict\n",
    "\n",
    "def splitDataNER(percentage): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        temp = preProcess(line)\n",
    "        if temp: #removes empty lines\n",
    "            trainDataBinary.append((temp,gender))\n",
    "            trainDataMulti.append((temp,character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        temp = preProcess(line)\n",
    "        if temp: #removes empty lines\n",
    "            testDataBinary.append((temp,gender))\n",
    "            testDataMulti.append((temp,character))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #training\n",
    "        classifier = trainClassifier(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabel(i,classifier))\n",
    "        \n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,average='macro') #finds precision score\n",
    "        #print(precision)\n",
    "        recall = Skmet.recall_score(trueLabels, results,average='macro') #finds recall score\n",
    "        #print(recall)\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        #print(f_score)\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        #print(accuracy)\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results\n",
    "\n",
    "def predictLabels(lineSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), lineSamples))\n",
    "\n",
    "def predictLabel(lineSample, classifier):\n",
    "    return classifier.classify((lineSample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData, 0 trainData, 0 testData\n",
      "Preparing the datasets...\n",
      "Now 9524 rawData, 0 trainData, 0 testData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Time to build vocab: 0.0 mins\n",
      "Time to train the model: 0.01 mins\n",
      "Now 9524 rawData, 8570 trainData, 954 testData, 8570 trainData, 954 testData\n",
      "Training Samples: \n",
      "8570\n",
      "Features: \n",
      "0\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Precision, Recall, Fscore, Accuracy\n",
      "(0.24848308051341889, 0.5, 0.3318746205492562, 0.49696616102683777)\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Precision, Recall, Fscore, Accuracy\n",
      "(0.006819655127706469, 0.055555555555555566, 0.012133636716618287, 0.12275379229871645)\n"
     ]
    }
   ],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "contractions = {} #dictionary of english contractions\n",
    "rawData = []          # the filtered data from the dataset file\n",
    "preprocessedData = [] # the preprocessed reviews \n",
    "trainDataBinary = []   # the training data with the binary gender labels.\n",
    "testDataBinary = [] # the test data currently 10% with the binary gender labels.\n",
    "trainDataMulti = [] # the training data with the multi class name labels.\n",
    "testDataMulti = [] # the test data currently 10% with the multi class name labels.\n",
    "\n",
    "trainingDataSource = \"training.csv\"\n",
    "testingDataSource = \"test.csv\"\n",
    "contractionsPath = \"contractions.txt\"\n",
    "loadContractions(contractionsPath)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData, %d trainData, %d testData\" % (len(rawData), len(trainDataMulti),\n",
    "    len(testDataMulti),len(trainDataBinary), len(testDataBinary)),\"Preparing the datasets...\",sep='\\n')\n",
    "\n",
    "#populating preprocessedData and rawData\n",
    "loadData(trainingDataSource)  \n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData, %d trainData, %d testData\" % (len(rawData), len(trainDataMulti),\n",
    "    len(testDataMulti),len(trainDataBinary), len(testDataBinary)), \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "splitDataNER(0.9)\n",
    "\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData, %d trainData, %d testData\" % (len(rawData), len(trainDataMulti),\n",
    "      len(testDataMulti),len(trainDataBinary), len(testDataBinary)), \"Training Samples: \", len(trainDataBinary), \n",
    "      \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "crossValidationBinaryResults = crossValidate(trainDataBinary, 10)\n",
    "print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "print(crossValidationBinaryResults)\n",
    "\n",
    "crossValidationMultiResults = crossValidate(trainDataMulti, 10)\n",
    "print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "print(crossValidationMultiResults)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are the results on a simple SVM \n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.552863688427898, 0.5525422108455417, 0.5527028606377715, 0.5524175824175823)\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.17559268036085093, 0.1614969298891055, 0.16823929530879572, 0.19098901098901097)\n",
    "##### Results with the use of sentence vector averages features using word2vec (Defunct)\n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.26970308788598574, 0.55, 0.36155444681963606, 0.5394061757719715)\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.009647928213248879, 0.07555555555555557, 0.016884015118703227, 0.1216627078384798)  #poor\n",
    "##### Here are the results with an addition of POS with stopword removal.\n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.5472277118296398, 0.5466568140554461, 0.5469418653347782, 0.5463736263736263)\n",
    "\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.1626678821084411, 0.15166406188712622, 0.15691284279441098, 0.18142857142857144)\n",
    "\n",
    "##### Here are the results with an addition of POS without stopword removal.\n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.5472277118296398, 0.5466568140554461, 0.5469418653347782, 0.5463736263736263)\n",
    "\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.16033137908389353, 0.15353047359820343, 0.15683540884530806, 0.17824175824175828)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defunct Word2Vec Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n",
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "model = word2vecVocabandTraining(preprocessedData, word2vecInitModel())\n",
    "splitDataW2V(0.9, model)\n",
    "\n",
    "def predictLabelW2V(lineSample, classifier, word_vectors):\n",
    "    temp = np.zeros(50)\n",
    "    count = 0;\n",
    "    for token in lineSample: #adding to the line dict\n",
    "        if token in word_vectors:\n",
    "            temp = temp + word_vectors[token]\n",
    "            count += 1;\n",
    "\n",
    "    if count > 0:\n",
    "        lineVector = np.divide(temp, count)    \n",
    "    else:\n",
    "        lineVector = temp;\n",
    "    return classifier.predict((lineVector.reshape(1, -1)))\n",
    "\n",
    "def word2vecInitModel():\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    word2vec = Word2Vec(min_count=10, #the kaggle tutorial used a much larger dataset \n",
    "                     window=2,\n",
    "                     size=50,\n",
    "                     sample=4e-6, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "    return word2vec\n",
    "\n",
    "def word2vecVocabandTraining(dataset, model):\n",
    "    t = time.time()\n",
    "    datasetSansLabels = list(map(itemgetter(0),dataset))\n",
    "    model.build_vocab(datasetSansLabels, progress_per=2000)\n",
    "    print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "    \n",
    "    t = time.time();\n",
    "    model.train(datasetSansLabels, total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "    print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "    return model\n",
    "\n",
    "def crossValidateW2V(dataset, folds, model):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #training\n",
    "        classifier = trainClassifierW2V(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabelW2V(i,classifier,model))\n",
    "        \n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,average='macro') #finds precision score\n",
    "        #print(precision)\n",
    "        recall = Skmet.recall_score(trueLabels, results,average='macro') #finds recall score\n",
    "        #print(recall)\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        #print(f_score)\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        #print(accuracy)\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results\n",
    "\n",
    "def trainClassifierW2V(trainData):\n",
    "    print(\"Training Classifier... SVC on vectors\")\n",
    "    SVC = LinearSVC()\n",
    "    X,Y = [ a for a,b in trainData ], [ b for a,b in trainData ]\n",
    "    return SVC.fit(X,Y)\n",
    "\n",
    "def splitDataW2V(percentage, model): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        temp = toFeatureVectorW2V(preProcess(line), model);\n",
    "        if type(temp) is not int:\n",
    "            trainDataBinary.append((temp, gender))\n",
    "            trainDataMulti.append((temp, character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        temp = toFeatureVectorW2V(preProcess(line), model);\n",
    "        if type(temp) is not int:\n",
    "            testDataBinary.append((temp, gender))\n",
    "            testDataMulti.append((temp,character))\n",
    "\n",
    "def toFeatureVectorW2V(tokens, word_vectors):\n",
    "    lineVector = []\n",
    "    temp = np.zeros(50)\n",
    "    count = 0;\n",
    "    for token in tokens: #adding to the line dict\n",
    "        if token in word_vectors:\n",
    "            temp = temp + word_vectors[token]\n",
    "            count += 1;\n",
    "    \n",
    "    lineVector = -1\n",
    "    if count > 0:\n",
    "        lineVector = np.divide(temp, count)\n",
    "    \n",
    "    return lineVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
