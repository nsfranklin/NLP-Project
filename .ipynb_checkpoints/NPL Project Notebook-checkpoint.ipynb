{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPL Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries, Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to c:/nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to c:/nltk_data/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv # csv reader\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as Skmet #used for precision_recall_fscore_support()\n",
    "from operator import itemgetter #used to unpack turples\n",
    "import re\n",
    "import math\n",
    "import multiprocessing\n",
    "#from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download(\"wordnet\", \"c:/nltk_data/\")\n",
    "nltk.download('stopwords', \"c:/nltk_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(dataLine):\n",
    "    line = dataLine[0]     \n",
    "    character = dataLine[1] \n",
    "    gender = dataLine[2]   \n",
    "    return (line, character, gender)\n",
    "\n",
    "def parseContractions(dataLine):\n",
    "    contraction = dataLine[0]     \n",
    "    expansion = dataLine[1]  \n",
    "    return (contraction.lower(), expansion.lower())\n",
    "\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = text.lower() #normalize the text \n",
    "    text = re.sub(r\"\\\"\", \"\", text) # speach mark removal.\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[']?\\w+[']?\\w?\\w?\\w?\\w?\\w?\") # splits sent into words maintain  \" ' \" to allow for later expansion.\n",
    "    tokenizedTemp = tokenizer.tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english')) #stop word removal.\n",
    "    \n",
    "    contractionExpandedTemp = []\n",
    "    for w in tokenizedTemp:\n",
    "        contemp = contractions.get(w)\n",
    "        if  contemp != None:\n",
    "            tempContract = contractions[w] #getting contraction dictionary value\n",
    "            expandedContraction = tempContract.split(\" \") #splits expanded cotnractions then adds it to the processed text.\n",
    "            contractionExpandedTemp.extend(expandedContraction);\n",
    "        else:\n",
    "            contractionExpandedTemp.append(w)\n",
    "    \n",
    "    #processedTemp = []\n",
    "    #for w in contractionExpandedTemp: #stop word removal and lemmatization\n",
    "    #    if (w not in stopWords) or w == \"\": # removes stop words and blank strings\n",
    "    #        processedTemp.append(w)   \n",
    "    \n",
    "    taggedTemp = nltk.pos_tag(contractionExpandedTemp)\n",
    "\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\" #nltk documentation.\n",
    "    \n",
    "    if taggedTemp:\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        result = cp.parse(taggedTemp)\n",
    "        return result\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for line in reader:\n",
    "            (line, character, gender) = parseData(line)\n",
    "            temp = preProcess(line)\n",
    "            if temp: #removes empty lines\n",
    "                rawData.append((line, character, gender));  \n",
    "                preprocessedData.append((temp, character, gender))\n",
    "\n",
    "def loadContractions(path):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            (contraction, expansion) = parseContractions(line)\n",
    "            contractions[contraction] = expansion\n",
    "            \n",
    "def splitData(percentage): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "        \n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "        \n",
    "def toFeatureVector(tokens):\n",
    "    lineDict = {} # Should return a dictionary containing features as keys, and weights as values\n",
    "    for token in tokens: #adding to the line dict\n",
    "        if token in lineDict:\n",
    "            lineDict[token] = lineDict[token] + 1\n",
    "        else:\n",
    "            lineDict[token] = 1\n",
    "            \n",
    "    for token in tokens: #adds values to featureDict\n",
    "        if token in featureDict:\n",
    "            featureDict[token] = featureDict[token] + 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "        \n",
    "    return lineDict\n",
    "\n",
    "def splitDataNER(percentage): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        temp = preProcess(line)\n",
    "        if temp: #removes empty lines\n",
    "            trainDataBinary.append((temp,gender))\n",
    "            trainDataMulti.append((temp,character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        temp = preProcess(line)\n",
    "        if temp: #removes empty lines\n",
    "            testDataBinary.append((temp,gender))\n",
    "            testDataMulti.append((temp,character))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #training\n",
    "        classifier = trainClassifier(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabel(i,classifier))\n",
    "        \n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,average='macro') #finds precision score\n",
    "        #print(precision)\n",
    "        recall = Skmet.recall_score(trueLabels, results,average='macro') #finds recall score\n",
    "        #print(recall)\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        #print(f_score)\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        #print(accuracy)\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results\n",
    "\n",
    "def predictLabels(lineSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), lineSamples))\n",
    "\n",
    "def predictLabel(lineSample, classifier):\n",
    "    return classifier.classify((lineSample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData, 0 trainData, 0 testData\n",
      "Preparing the datasets...\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "Now 10067 rawData, 0 trainData, 0 testData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b0a980a1abc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     len(testDataMulti),len(trainDataBinary), len(testDataBinary)), \"Preparing training and test data...\",sep='\\n')\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0msplitDataNER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# We print the number of training samples and the number of features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-71242e20a3a7>\u001b[0m in \u001b[0;36msplitDataNER\u001b[1;34m(percentage)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mtrainingSamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataSamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharacter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrawData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrainingSamples\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrawData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhalfOfData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhalfOfData\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtrainingSamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#removes empty lines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mtrainDataBinary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-71242e20a3a7>\u001b[0m in \u001b[0;36mpreProcess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtokenizedTemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mstopWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#stop word removal.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mcontractionExpandedTemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     23\u001b[0m         return [\n\u001b[0;32m     24\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such file or directory: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "contractions = {} #dictionary of english contractions\n",
    "rawData = []          # the filtered data from the dataset file\n",
    "preprocessedData = [] # the preprocessed reviews \n",
    "trainDataBinary = []   # the training data with the binary gender labels.\n",
    "testDataBinary = [] # the test data currently 10% with the binary gender labels.\n",
    "trainDataMulti = [] # the training data with the multi class name labels.\n",
    "testDataMulti = [] # the test data currently 10% with the multi class name labels.\n",
    "\n",
    "trainingDataSource = \"training.csv\"\n",
    "testingDataSource = \"test.csv\"\n",
    "contractionsPath = \"contractions.txt\"\n",
    "loadContractions(contractionsPath)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData, %d trainData, %d testData\" % (len(rawData), len(trainDataMulti),\n",
    "    len(testDataMulti),len(trainDataBinary), len(testDataBinary)),\"Preparing the datasets...\",sep='\\n')\n",
    "\n",
    "#populating preprocessedData and rawData\n",
    "loadData(trainingDataSource)  \n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData, %d trainData, %d testData\" % (len(rawData), len(trainDataMulti),\n",
    "    len(testDataMulti),len(trainDataBinary), len(testDataBinary)), \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "splitDataNER(0.9)\n",
    "\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData, %d trainData, %d testData\" % (len(rawData), len(trainDataMulti),\n",
    "      len(testDataMulti),len(trainDataBinary), len(testDataBinary)), \"Training Samples: \", len(trainDataBinary), \n",
    "      \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "crossValidationBinaryResults = crossValidate(trainDataBinary, 10)\n",
    "print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "print(crossValidationBinaryResults)\n",
    "\n",
    "crossValidationMultiResults = crossValidate(trainDataMulti, 10)\n",
    "print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "print(crossValidationMultiResults)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print((model.wv.vocab))\n",
    "print(model.wv.most_similar(positive=[\"sean\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are the results on a simple SVM \n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.552863688427898, 0.5525422108455417, 0.5527028606377715, 0.5524175824175823)\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.17559268036085093, 0.1614969298891055, 0.16823929530879572, 0.19098901098901097)\n",
    "##### Results with the use of sentence vector averages features using word2vec (Defunct)\n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.26970308788598574, 0.55, 0.36155444681963606, 0.5394061757719715)\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.009647928213248879, 0.07555555555555557, 0.016884015118703227, 0.1216627078384798)  #poor\n",
    "##### Here are the results with an addition of POS with stopword removal.\n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.5472277118296398, 0.5466568140554461, 0.5469418653347782, 0.5463736263736263)\n",
    "\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.1626678821084411, 0.15166406188712622, 0.15691284279441098, 0.18142857142857144)\n",
    "\n",
    "##### Here are the results with an addition of POS without stopword removal.\n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.5472277118296398, 0.5466568140554461, 0.5469418653347782, 0.5463736263736263)\n",
    "\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.16033137908389353, 0.15353047359820343, 0.15683540884530806, 0.17824175824175828)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defunct Word2Vec Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n",
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "model = word2vecVocabandTraining(preprocessedData, word2vecInitModel())\n",
    "splitDataW2V(0.9, model)\n",
    "\n",
    "def predictLabelW2V(lineSample, classifier, word_vectors):\n",
    "    temp = np.zeros(50)\n",
    "    count = 0;\n",
    "    for token in lineSample: #adding to the line dict\n",
    "        if token in word_vectors:\n",
    "            temp = temp + word_vectors[token]\n",
    "            count += 1;\n",
    "\n",
    "    if count > 0:\n",
    "        lineVector = np.divide(temp, count)    \n",
    "    else:\n",
    "        lineVector = temp;\n",
    "    return classifier.predict((lineVector.reshape(1, -1)))\n",
    "\n",
    "def word2vecInitModel():\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    word2vec = Word2Vec(min_count=10, #the kaggle tutorial used a much larger dataset \n",
    "                     window=2,\n",
    "                     size=50,\n",
    "                     sample=4e-6, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "    return word2vec\n",
    "\n",
    "def word2vecVocabandTraining(dataset, model):\n",
    "    t = time.time()\n",
    "    datasetSansLabels = list(map(itemgetter(0),dataset))\n",
    "    model.build_vocab(datasetSansLabels, progress_per=2000)\n",
    "    print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "    \n",
    "    t = time.time();\n",
    "    model.train(datasetSansLabels, total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "    print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "    return model\n",
    "\n",
    "def crossValidateW2V(dataset, folds, model):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #training\n",
    "        classifier = trainClassifierW2V(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabelW2V(i,classifier,model))\n",
    "        \n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,average='macro') #finds precision score\n",
    "        #print(precision)\n",
    "        recall = Skmet.recall_score(trueLabels, results,average='macro') #finds recall score\n",
    "        #print(recall)\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        #print(f_score)\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        #print(accuracy)\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results\n",
    "\n",
    "def trainClassifierW2V(trainData):\n",
    "    print(\"Training Classifier... SVC on vectors\")\n",
    "    SVC = LinearSVC()\n",
    "    X,Y = [ a for a,b in trainData ], [ b for a,b in trainData ]\n",
    "    return SVC.fit(X,Y)\n",
    "\n",
    "def splitDataW2V(percentage, model): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        temp = toFeatureVectorW2V(preProcess(line), model);\n",
    "        if type(temp) is not int:\n",
    "            trainDataBinary.append((temp, gender))\n",
    "            trainDataMulti.append((temp, character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        temp = toFeatureVectorW2V(preProcess(line), model);\n",
    "        if type(temp) is not int:\n",
    "            testDataBinary.append((temp, gender))\n",
    "            testDataMulti.append((temp,character))\n",
    "\n",
    "def toFeatureVectorW2V(tokens, word_vectors):\n",
    "    lineVector = []\n",
    "    temp = np.zeros(50)\n",
    "    count = 0;\n",
    "    for token in tokens: #adding to the line dict\n",
    "        if token in word_vectors:\n",
    "            temp = temp + word_vectors[token]\n",
    "            count += 1;\n",
    "    \n",
    "    lineVector = -1\n",
    "    if count > 0:\n",
    "        lineVector = np.divide(temp, count)\n",
    "    \n",
    "    return lineVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
