{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPL Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries, Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # csv reader\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as Skmet #used for precision_recall_fscore_support()\n",
    "from operator import itemgetter #used to unpack turples\n",
    "import re\n",
    "import math\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(dataLine):\n",
    "    line = dataLine[0]     \n",
    "    character = dataLine[1] \n",
    "    gender = dataLine[2]   \n",
    "    return (line, character, gender)\n",
    "\n",
    "def parseContractions(dataLine):\n",
    "    contraction = dataLine[0]     \n",
    "    expansion = dataLine[1]    \n",
    "    return (contraction.lower(), expansion.lower())\n",
    "\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = text.lower() #normalize the text \n",
    "    text = re.sub(r\"\\\"\", \"\", text) # speach mark removal.\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[']?\\w+[']?\\w?\\w?\\w?\\w?\\w?\") # splits sent into words maintain  \" ' \" to allow for later expansion.\n",
    "    tokenizedTemp = tokenizer.tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english')) #stop word removal.\n",
    "    \n",
    "    contractionExpandedTemp = []\n",
    "    for w in tokenizedTemp:\n",
    "        contemp = contractions.get(w)\n",
    "        if  contemp != None:\n",
    "            tempContract = contractions[w] #getting contraction dictionary value\n",
    "            expandedContraction = tempContract.split(\" \") #splits expanded cotnractions then adds it to the processed text.\n",
    "            contractionExpandedTemp.extend(expandedContraction);\n",
    "        else:\n",
    "            contractionExpandedTemp.append(w)\n",
    "    processedTemp = []\n",
    "    \n",
    "    for w in contractionExpandedTemp: #stop word removal and lemmatization\n",
    "        if (w not in stopWords) or w == \"\": # removes stop words and blank strings\n",
    "            processedTemp.append(w)  #lemmatizer.lemmatize(w))   \n",
    "    return processedTemp\n",
    "\n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for line in reader:\n",
    "            (line, character, gender) = parseData(line)\n",
    "            temp = preProcess(line)\n",
    "            #if not temp: #removes empty lines. Either fully trimmed or original empty.\n",
    "            rawData.append((line, character, gender));  \n",
    "            preprocessedData.append((preProcess(line), character, gender))\n",
    "                \n",
    "def loadContractions(path):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            (contraction, expansion) = parseContractions(line)\n",
    "            contractions[contraction] = expansion\n",
    "            \n",
    "def splitData(percentage): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "        \n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "        \n",
    "def toFeatureVector(tokens):\n",
    "    lineDict = {} # Should return a dictionary containing features as keys, and weights as values\n",
    "    for token in tokens: #adding to the line dict\n",
    "        if token in lineDict:\n",
    "            lineDict[token] = lineDict[token] + 1\n",
    "        else:\n",
    "            lineDict[token] = 1\n",
    "            \n",
    "    for token in tokens: #adds values to featureDict\n",
    "        if token in featureDict:\n",
    "            featureDict[token] = featureDict[token] + 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "    \n",
    "    return lineDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #training\n",
    "        classifier = trainClassifier(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabel(i,classifier))\n",
    "        \n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,average='macro') #finds precision score\n",
    "        #print(precision)\n",
    "        recall = Skmet.recall_score(trueLabels, results,average='macro') #finds recall score\n",
    "        #print(recall)\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        #print(f_score)\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        #print(accuracy)\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results\n",
    "\n",
    "def predictLabels(lineSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), lineSamples))\n",
    "\n",
    "def predictLabel(lineSample, classifier):\n",
    "    return classifier.classify((lineSample))\n",
    "\n",
    "def word2vecInitModel():\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    word2vec = Word2Vec(min_count=3, #the kaggle tutorial used a much larger dataset \n",
    "                     window=2,\n",
    "                     size=100,\n",
    "                     sample=4e-6, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "    return word2vec\n",
    "\n",
    "def word2vecVocabandTraining(dataset, model):\n",
    "    t = time.time()\n",
    "    datasetSansLabels = list(map(itemgetter(0),dataset))\n",
    "    model.build_vocab(datasetSansLabels, progress_per=2000)\n",
    "    print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "    \n",
    "    t = time.time();\n",
    "    model.train(datasetSansLabels, total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "    print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "    return model\n",
    "\n",
    "def crossValidateWithW2V(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        model = word2vecVocabandTraining(tempDataSet, word2vecInitModel());\n",
    "        word_vectors = model.wv\n",
    "        \n",
    "        #transforming the word 2 vector into something useful.\n",
    "        #creating sentence vectors using the word vector model.\n",
    "        \n",
    "        \n",
    "        sentence_vectors_with_labels = create_sentence_vectors() \n",
    "        \n",
    "        #training\n",
    "        classifier = trainClassifier() #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabel(i,classifier))\n",
    "        \n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,average='macro') #finds precision score\n",
    "        #print(precision)\n",
    "        recall = Skmet.recall_score(trueLabels, results,average='macro') #finds recall score\n",
    "        #print(recall)\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        #print(f_score)\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        #print(accuracy)\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to c:/nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to c:/nltk_data/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData, 0 trainData, 0 testData\n",
      "Preparing the datasets...\n",
      "Now 10113 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 10113 rawData, 9100 trainData, 1013 testData\n",
      "Training Samples: \n",
      "9100\n",
      "Features: \n",
      "5754\n",
      "Time to build vocab: 0.0 mins\n",
      "Time to train the model: 0.01 mins\n",
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000021A31E04548>\n",
      "Training Classifier...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-84cf30ebbf1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#print(crossValidationMultiResults)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mcrossValidationBinaryResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossValidateWithW2V\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataBinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-598438ffedd2>\u001b[0m in \u001b[0;36mcrossValidateWithW2V\u001b[1;34m(dataset, folds)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m#training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#classifier using 9/10th of the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[1;31m#classifing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-598438ffedd2>\u001b[0m in \u001b[0;36mtrainClassifier\u001b[1;34m(trainData)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Classifier...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mSklearnClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcrossValidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\scikitlearn.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, labeled_featuresets)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \"\"\"\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "contractions = {} #dictionary of english contractions\n",
    "rawData = []          # the filtered data from the dataset file\n",
    "preprocessedData = [] # the preprocessed reviews \n",
    "trainDataBinary = []   # the training data with the binary gender labels.\n",
    "testDataBinary = [] # the test data currently 10% with the binary gender labels.\n",
    "trainDataMulti = [] # the training data with the multi class name labels.\n",
    "testDataMulti = [] # the test data currently 10% with the multi class name labels.\n",
    "\n",
    "trainingDataSource = \"training.csv\"\n",
    "testingDataSource = \"test.csv\"\n",
    "contractionsPath = \"contractions.txt\"\n",
    "loadContractions(contractionsPath)\n",
    "nltk.download(\"wordnet\", \"c:/nltk_data/\")\n",
    "nltk.download('stopwords', \"c:/nltk_data/\")\n",
    "\n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData, %d trainData, %d testData\" % (len(rawData), len(trainDataMulti), len(testDataMulti),len(trainDataBinary), len(testDataBinary)),\"Preparing the datasets...\",sep='\\n')\n",
    "loadData(trainingDataSource)  \n",
    "\n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainDataBinary), len(testDataBinary)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.9)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainDataBinary), len(testDataBinary)),\n",
    "      \"Training Samples: \", len(trainDataBinary), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "#crossValidationBinaryResults = crossValidate(trainDataBinary, 10)\n",
    "#print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "#print(crossValidationBinaryResults)\n",
    "\n",
    "#crossValidationMultiResults = crossValidate(trainDataMulti, 10)\n",
    "\n",
    "#print(\"Precision, Recall, Fscore, Accuracy\")\n",
    "#print(crossValidationMultiResults)\n",
    "\n",
    "crossValidationBinaryResults = crossValidateWithW2V(trainDataBinary, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 0.98886638879776), ('like', 0.9887083172798157), ('got', 0.9886667728424072), ('back', 0.988623321056366), ('want', 0.9886059761047363), ('going', 0.988605260848999), ('come', 0.9886035919189453), ('one', 0.9882631301879883), ('know', 0.9881845712661743), ('day', 0.9881817102432251)]\n"
     ]
    }
   ],
   "source": [
    "#print((model.wv.vocab))\n",
    "print(model.wv.most_similar(positive=[\"sean\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here are the results on a simple SVM \n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.552863688427898, 0.5525422108455417, 0.5527028606377715, 0.5524175824175823)\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "(0.17559268036085093, 0.1614969298891055, 0.16823929530879572, 0.19098901098901097)\n",
    "##### Here are the results with the edition of Word2Vec Features\n",
    "###### Binary (Gender)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "\n",
    "###### Multiclass (Name)\n",
    "Precision, Recall, Fscore, Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
