{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPL Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries, Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # csv reader\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as Skmet #used for precision_recall_fscore_support()\n",
    "from operator import itemgetter #used to unpack turples\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Splitting dataset\n"
     ]
    }
   ],
   "source": [
    "def parseData(dataLine):\n",
    "    line = dataLine[0]     \n",
    "    character = dataLine[1] \n",
    "    gender = dataLine[2]   \n",
    "    return (line, character, gender)\n",
    "\n",
    "def parseContractions(dataLine):\n",
    "    contraction = dataLine[0]     \n",
    "    expansion = dataLine[1]    \n",
    "    return (contraction.lower(), expansion.lower())\n",
    "\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = text.lower() #normalize the text \n",
    "    text = re.sub(r\"\\\"\", \"\", text) # speach mark removal.\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[']?\\w+[']?\\w?\\w?\") # splits sent into words maintain  \" ' \" to allow for later expansion.\n",
    "    tokenizedTemp = tokenizer.tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english')) #stop word removal.\n",
    "    \n",
    "    contractionExpandedTemp = []\n",
    "    for w in tokenizedTemp:\n",
    "        contemp = contractions.get(w)\n",
    "        if  contemp != None:\n",
    "            tempContract = contractions[w] #getting contraction dictionary value\n",
    "            expandedContraction = tempContract.split(\" \") #splits expanded cotnractions then adds it to the processed text.\n",
    "            contractionExpandedTemp.extend(expandedContraction);\n",
    "        else:\n",
    "            contractionExpandedTemp.append(w)\n",
    "    processedTemp = []\n",
    "    \n",
    "    for w in contractionExpandedTemp: #stop word removal and lemmatization\n",
    "        if (w not in stopWords) or w == \"\": # removes stop words and blank strings\n",
    "            processedTemp.append(w)  #lemmatizer.lemmatize(w))   \n",
    "    return processedTemp\n",
    "\n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for line in reader:\n",
    "            (line, character, gender) = parseData(line)\n",
    "            temp = preProcess(line)\n",
    "            if not temp: #removes empty lines. Either fully trimmed or original empty.\n",
    "                rawData.append((line, character, gender));              \n",
    "                preprocessedData.append((preProcess(line), character, gender))\n",
    "                \n",
    "def loadContractions(path):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            (contraction, expansion) = parseContractions(line)\n",
    "            contractions[contraction] = expansion\n",
    "            \n",
    "def splitData(percentage): #splits the data into 4 sets\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    \n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataBinary.append((toFeatureVector(preProcess(line)),gender))\n",
    "        \n",
    "    for (line, character, gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "    \n",
    "    for (line, character, gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testDataMulti.append((toFeatureVector(preProcess(line)),character))\n",
    "        \n",
    "def toFeatureVector(tokens):\n",
    "    lineDict = {} # Should return a dictionary containing features as keys, and weights as values\n",
    "    \n",
    "    for token in tokens: #adding to the line dict\n",
    "        if token in lineDict:\n",
    "            lineDict[token] = lineDict[token] + 1\n",
    "        else:\n",
    "            lineDict[token] = 1\n",
    "            \n",
    "    for token in tokens: #adds values to featureDict\n",
    "        if token in featureDict:\n",
    "            featureDict[token] = featureDict[token] + 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "    \n",
    "    return lineDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    #print(dataset[0])\n",
    "    cv_results = []\n",
    "    temp = []\n",
    "    totalPrecision = 0\n",
    "    totalRecall = 0\n",
    "    totalFScore = 0\n",
    "    totalAccuracy = 0\n",
    "    foldSize = int(len(dataset)/folds) #the fold size\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        tempDataSet = dataset[0:i] + dataset[i+foldSize:len(dataset)] # joins to parts of the list to form the dataset to test.\n",
    "        testingSet = dataset[i:i+foldSize] #takes the fold size an i to find the current test data.\n",
    "        trueLabels = list(map(itemgetter(1), testingSet)) #creates a 1D array of result labels\n",
    "        testingSetRemovedLabel = list(map(itemgetter(0),testingSet))\n",
    "        \n",
    "        #print(testingSetRemovedLabel)\n",
    "        #training\n",
    "        classifier = trainClassifier(tempDataSet) #classifier using 9/10th of the dataset\n",
    "        #classifing\n",
    "        \n",
    "        results = []\n",
    "        for i in testingSetRemovedLabel:\n",
    "            results.append(predictLabel(i,classifier))\n",
    "        \n",
    "        #predictLabels using 1/10th of the dataset\n",
    "        #print(results[0:10])        \n",
    "        #print(trueLabels[0:10])\n",
    "        \n",
    "        precision = Skmet.precision_score(trueLabels, results,pos_label=\"fake\") #finds precision score\n",
    "        recall = Skmet.recall_score(trueLabels, results,pos_label=\"fake\") #finds recall score\n",
    "        f_score = 2 * (precision * recall)/(precision + recall) #calculates f_score\n",
    "        accuracy = Skmet.accuracy_score(trueLabels,results) #calculate accuracy\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "        if math.isnan(f_score): #if f_score not a number won't add it to the total\n",
    "            totalFScore += 0\n",
    "        else:\n",
    "            totalFScore += f_score\n",
    "        totalAccuracy += accuracy\n",
    "    \n",
    "    cv_results = (totalPrecision/folds,totalRecall/folds,totalFScore/folds,totalAccuracy/folds)\n",
    "        \n",
    "    return cv_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Splitting dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'crossValidate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-0257a9c6ea21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0msplitData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mcrossValidationBinaryResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossValidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataBinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mcrossValidationMultiResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossValidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataMulti\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'crossValidate' is not defined"
     ]
    }
   ],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "contractions = {} #dictionary of english contractions\n",
    "rawData = []          # the filtered data from the dataset file\n",
    "preprocessedData = [] # the preprocessed reviews \n",
    "trainDataBinary = []   # the training data with the binary gender labels.\n",
    "testDataBinary = [] # the test data currently 10% with the binary gender labels.\n",
    "trainDataMulti = [] # the training data with the multi class name labels.\n",
    "testDataMulti = [] # the test data currently 10% with the multi class name labels.\n",
    "\n",
    "trainingDataSource = \"training.csv\"\n",
    "testingDataSource = \"test.csv\"\n",
    "contractionsPath = \"contractions.txt\"\n",
    "\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)), \"Preparing the dataset...\",sep='\\n')\n",
    "3\n",
    "loadContractions(contractionsPath)\n",
    "loadData(trainingDataSource)  \n",
    "\n",
    "print(\"Splitting dataset\")\n",
    "splitData(0.9)\n",
    "\n",
    "crossValidationBinaryResults = crossValidate(trainDataBinary, 10)\n",
    "\n",
    "crossValidationMultiResults = crossValidate(trainDataMulti, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
